{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from fer import FER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cv2: OpenCV library for image processing and computer vision tasks.\n",
    "2. numpy: Library for numerical operations.\n",
    "3. pygame: Library for audio playback.\n",
    "4. os: Module for interacting with the operating system.\n",
    "5. mediapipe: Library for detecting hand gestures.\n",
    "6. fer: Library for facial emotion recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Initialize Pygame Mixer for Audio Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initializes the Pygame mixer module, which is used for loading and playing music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Functions for Music Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_music(file_path):\n",
    "    pygame.mixer.music.load(file_path)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "def pause_music():\n",
    "    pygame.mixer.music.pause()\n",
    "\n",
    "def unpause_music():\n",
    "    pygame.mixer.music.unpause()\n",
    "\n",
    "def stop_music():\n",
    "    pygame.mixer.music.stop()\n",
    "\n",
    "def increase_volume():\n",
    "    current_volume = pygame.mixer.music.get_volume()\n",
    "    if current_volume <= 0.9:\n",
    "        pygame.mixer.music.set_volume(current_volume + 0.1)\n",
    "\n",
    "def decrease_volume():\n",
    "    current_volume = pygame.mixer.music.get_volume()\n",
    "    if current_volume >= 0.1:\n",
    "        pygame.mixer.music.set_volume(current_volume - 0.1)\n",
    "\n",
    "def next_song():\n",
    "    global current_song_index\n",
    "    current_song_index = (current_song_index + 1) % len(current_playlist)\n",
    "    play_music(current_playlist[current_song_index])\n",
    "\n",
    "def previous_song():\n",
    "    global current_song_index\n",
    "    current_song_index = (current_song_index - 1) % len(current_playlist)\n",
    "    play_music(current_playlist[current_song_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions handle various aspects of music playback, including playing, pausing, stopping, and changing the volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define Folder Paths for Playlists Based on Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_folder = \"E:/Song\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets the base directory where the music playlists are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Map Emotions to Playlist Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_playlist = {\n",
    "    \"happy\": os.path.join(playlist_folder, \"happy\"),\n",
    "    \"sad\": os.path.join(playlist_folder, \"sad\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Initialize MediaPipe Hands Model for Gesture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Initialize FER Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_detector = FER(mtcnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: OpenCV Setup for Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: sad\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Detect emotion\n",
    "detected_emotion = None\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Detect emotions in the frame\n",
    "    emotion_result = emotion_detector.detect_emotions(frame)\n",
    "    \n",
    "    if emotion_result:\n",
    "        dominant_emotion = max(emotion_result[0]['emotions'], key=emotion_result[0]['emotions'].get)\n",
    "        if dominant_emotion in [\"happy\", \"sad\"]:\n",
    "            detected_emotion = dominant_emotion\n",
    "            print(f\"Detected Emotion: {detected_emotion}\")\n",
    "            break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Check for key press events\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the emotion detection resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Load Playlist Based on Detected Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detected_emotion:\n",
    "    # Load playlist based on detected emotion\n",
    "    playlist_path = emotion_to_playlist[detected_emotion]\n",
    "    current_playlist = [os.path.join(playlist_path, file) for file in os.listdir(playlist_path) if file.endswith(\".mp3\")]\n",
    "    current_song_index = 0\n",
    "\n",
    "    # Play the first song from the playlist\n",
    "    if current_playlist:\n",
    "        play_music(current_playlist[current_song_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: OpenCV Setup for Music Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # OpenCV setup for music control\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Set a larger resolution for the capture (e.g., 1280x720)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    # Define control boxes positions based on the new resolution\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    boxes = {\n",
    "        \"play_pause\": (frame_width // 3, frame_height // 3, 2 * frame_width // 3, 2 * frame_height // 3),\n",
    "        \"next\": (2 * frame_width // 3 + 20, frame_height // 3, frame_width - 20, 2 * frame_height // 3),\n",
    "        \"previous\": (20, frame_height // 3, frame_width // 3 - 20, 2 * frame_height // 3),\n",
    "        \"increase_volume\": (frame_width // 3, 20, 2 * frame_width // 3, frame_height // 3 - 20),\n",
    "        \"decrease_volume\": (frame_width // 3, 2 * frame_height // 3 + 20, 2 * frame_width // 3, frame_height - 20),\n",
    "    }\n",
    "\n",
    "    # Instructions for each control box\n",
    "    instructions = {\n",
    "        \"play_pause\": \"Play/Pause\",\n",
    "        \"next\": \"Next Song\",\n",
    "        \"previous\": \"Previous Song\",\n",
    "        \"increase_volume\": \"Increase Volume\",\n",
    "        \"decrease_volume\": \"Decrease Volume\"\n",
    "    }\n",
    "\n",
    "    # Font properties for instructions\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.8\n",
    "    font_color = (255, 255, 255)\n",
    "    line_type = 2\n",
    "\n",
    "    # Gesture recognition loop\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect hands in the frame\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Draw control boxes and instructions\n",
    "        for key, (x1, y1, x2, y2) in boxes.items():\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            text_size = cv2.getTextSize(instructions[key], font, font_scale, line_type)[0]\n",
    "            text_x = x1 + (x2 - x1 - text_size[0]) // 2\n",
    "            text_y = y1 + (y2 - y1 + text_size[1]) // 2\n",
    "            cv2.putText(frame, instructions[key], (text_x, text_y), font, font_scale, font_color, line_type)\n",
    "\n",
    "        # Check if hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get the index finger tip landmark\n",
    "                index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "                # Get index finger tip coordinates\n",
    "                index_finger_x, index_finger_y = int(index_finger_tip.x * frame.shape[1]), int(index_finger_tip.y * frame.shape[0])\n",
    "\n",
    "                # Check which box the index finger is in\n",
    "                for key, (x1, y1, x2, y2) in boxes.items():\n",
    "                    if x1 <= index_finger_x <= x2 and y1 <= index_finger_y <= y2:\n",
    "                        if key == \"play_pause\":\n",
    "                            if pygame.mixer.music.get_busy():\n",
    "                                pause_music()\n",
    "                            else:\n",
    "                                play_music(current_playlist[current_song_index])\n",
    "                        elif key == \"next\":\n",
    "                            next_song()\n",
    "                        elif key == \"previous\":\n",
    "                            previous_song()\n",
    "                        elif key == \"increase_volume\":\n",
    "                            increase_volume()\n",
    "                        elif key == \"decrease_volume\":\n",
    "                            decrease_volume()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Gesture-Controlled Music Player\", frame)\n",
    "\n",
    "        # Check for key press events\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
